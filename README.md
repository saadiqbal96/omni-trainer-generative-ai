OmniTrainer: Multimodal Moderation Sandbox
------------------------------------------

**Author:** Saad Iqbal**Date:** 05/01/2026

**OmniTrainer** is an AI-powered multimodal content moderation sandbox built for a fictional **ACME Enterprise helpdesk**.The project simulates real-world customer support interactions where a trainee agent engages with an angry customer, while **all user inputs are moderated in real time** across **text, images, audio, and video** using Google Gemini-style LLMs.

The system exposes both an **interactive Gradio chat UI** and **programmatic FastAPI endpoints**, with full **OpenTelemetry tracing and Arize Phoenix observability** to inspect moderation behavior, conversation flow, and feedback signals.

Key Features
------------

*   âœ… **Structured moderation outputs** via a shared ModerationResult Pydantic model
    
*   ğŸ§  **LLM-based moderation agents** for text, image, audio, and video inputs
    
*   ğŸ’¬ **Simulated LLM customer agent** with contextual, stateful responses
    
*   ğŸ–¥ï¸ **Gradio multimodal chat UI** with pre-display moderation enforcement
    
*   ğŸŒ **FastAPI endpoints** for programmatic moderation access
    
*   ğŸ” **End-to-end tracing** using OpenTelemetry and Arize Phoenix
    
*   ğŸ§ª **Pydantic-based evals** and comprehensive unit test coverage
    

Repository Structure
--------------------

`   multimodal_moderation/  â”œâ”€â”€ agents/                  # Moderation agents (text, image, audio, video)  â”‚   â””â”€â”€ customer_agent.py    # Simulated LLM customer  â”œâ”€â”€ types/  â”‚   â””â”€â”€ moderation_result.py # Pydantic schema for structured moderation output  â”œâ”€â”€ gradio_app.py            # Multimodal chat UI with moderation hooks  â”œâ”€â”€ fastapi_app.py           # REST API for moderation services  â”œâ”€â”€ tracing.py               # OpenTelemetry + Phoenix tracing setup  â”œâ”€â”€ __main__.py              # Entrypoint to run all services together  evals/  â”œâ”€â”€ text/                    # Text moderation evaluation cases  â”œâ”€â”€ image/                   # Image moderation evaluation cases  â”œâ”€â”€ audio/                   # Audio moderation evaluation cases  â”œâ”€â”€ video/                   # Video moderation evaluation cases  tests/  â”œâ”€â”€ test_moderation_result.py  â”œâ”€â”€ test_text_agent.py  â”œâ”€â”€ test_image_agent.py  â”œâ”€â”€ test_gradio_app.py  example.env                  # Environment variable template  requirements.txt             # Full dependency set  requirements_slim.txt        # Minimal runtime dependencies  uv.lock                      # Locked dependency graph   `

Setup
-----

### 1\. Create and Activate a Virtual Environment

**Using uv (recommended):**

`   uv sync --dev  uv pip install -e .   `

**Using standard venv + pip:**

`   python -m venv .venv  source .venv/bin/activate  pip install -r requirements.txt  pip install -e .   `

### 2\. Configure Environment Variables

`   cp example.env .env   `

Edit .env and provide:

*   GEMINI\_API\_KEY â€“ API key for the Gemini-compatible LLM
    
*   USER\_API\_KEY â€“ arbitrary key used for FastAPI authentication
    

### 3\. (Optional) IDE Configuration

Select the virtual environment interpreter in your IDE to enable linting, type hints, and autocomplete.

Running the Project
-------------------

### Run All Tests

`   uv run pytest tests/ -vv   `

### Start the Full Application Stack

`   uv run multimodal-moderation   `

This launches:

*   **Gradio UI:** [http://localhost:7860](http://localhost:7860)
    
*   **FastAPI Docs:** [http://0.0.0.0:8000/docs](http://0.0.0.0:8000/docs)
    
*   **Phoenix Dashboard:** [http://localhost:6006/projects](http://localhost:6006/projects)
